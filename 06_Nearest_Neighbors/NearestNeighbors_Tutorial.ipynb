{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NearestNeighbors_Tutorial",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S36f2M-S6bsV",
        "colab_type": "text"
      },
      "source": [
        "# Nearest Neighbors Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xb0q8y0rq6Ve"
      },
      "source": [
        "The basic concept for **Nearest Neighbors Algorithms** (NNAs) is to classify a datum by finding one or more points that have similar features. The most similar points are called the **nearest neighbors**. Once found, the input datum can be put in the same class as its neighbors. We can also use this to predict the value of missing features for the input datum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PdwUJygSGU9F"
      },
      "source": [
        "## k-Nearest Neighbors\n",
        "\n",
        "The most commonly used NNA is **k-Nearest Neighbors,** in which the top $k$ nearest neighbors (best matches) are identified. In most instantiations of k-NNA, classification or prediction is based on a **majority vote** of the $k$ nearest neighbors.\n",
        ">For example, if $k = 5$, and at least 3 out of the 5 nearest neighbors to an input datum are class A, then we would assign the new datum to class A.\n",
        "\n",
        "For a more complex example, see the image below. Here, if $k = 1$, the green circle would be assigned to Class 1, since the nearest point is a blue square. However, if $k = 3$ the answer becomes Class 2, since the next two closest are both red triangles.\n",
        "\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/0*Sk18h9op6uK9EpT8.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iZrldzQgy9ud"
      },
      "source": [
        "## Quick example to illustrate a kNN:\n",
        "\n",
        "In this section, we will use sklearn to build a k-NNA model for a data set describing cars. Our goal will be to classify the cars into one of two categories: \"cool\" or \"uncool.\" Clearly these are subjective terms, but that's ok: we will provide a manually classified training set.\n",
        "\n",
        "For example, if we consider the variables *horsepower, number of seats,* and *manual (0) or automatic (1)*, our manually classified training set might look like this:\n",
        "\n",
        "*   150, 5, 0, uncool (2008 Honda Civic)  \n",
        "*   320, 5, 0, cool (2011 Dodge Charger)\n",
        "*   383, 3, 1, cool (1985 Chevy Blazer)\n",
        "*   210, 7, 0, uncool (2001 Honda Odyssey)\n",
        "\n",
        "Let's say we're trying to predict whether the 2017 Bugatti Veyron (1500hp, 2 seats, manual: 1) is cool or not. Our first step is to load the data into a python structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p64sH6r76vWE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX6Uc4e-61NI",
        "colab_type": "text"
      },
      "source": [
        "### Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s4VMsygjy9ue",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "4b0d7029-00a0-4c7f-e807-b1a9cd19d3d5"
      },
      "source": [
        "cars_dict = {'2008 Honda Civic':    {'hp':150., 'seats':5., 'auto':0., 'cool':0}, \n",
        "             '2011 Dodge Charger' : {'hp':320., 'seats':5., 'auto':0., 'cool':1}, \n",
        "             '1985 Chevy Blazer':   {'hp':383., 'seats':3., 'auto':1., 'cool':1}, \n",
        "             '2001 Honda Odyssey':  {'hp':210., 'seats':7., 'auto':0., 'cool':0}, \n",
        "             '2017 Bugatti Veyron': {'hp':1500.,'seats':2., 'auto':1., 'cool':None}}\n",
        "\n",
        "data = pd.DataFrame.from_dict(cars_dict,orient='index')\n",
        "data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hp</th>\n",
              "      <th>seats</th>\n",
              "      <th>auto</th>\n",
              "      <th>cool</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2008 Honda Civic</th>\n",
              "      <td>150.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2011 Dodge Charger</th>\n",
              "      <td>320.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985 Chevy Blazer</th>\n",
              "      <td>383.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2001 Honda Odyssey</th>\n",
              "      <td>210.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017 Bugatti Veyron</th>\n",
              "      <td>1500.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         hp  seats  auto  cool\n",
              "2008 Honda Civic      150.0    5.0   0.0   0.0\n",
              "2011 Dodge Charger    320.0    5.0   0.0   1.0\n",
              "1985 Chevy Blazer     383.0    3.0   1.0   1.0\n",
              "2001 Honda Odyssey    210.0    7.0   0.0   0.0\n",
              "2017 Bugatti Veyron  1500.0    2.0   1.0   NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezEwHH-x67cj",
        "colab_type": "text"
      },
      "source": [
        "### Normalizing Data and Calculating Distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5STDuS1DF0PA"
      },
      "source": [
        "To determine the class of our Bugatti using NNA, we need to measure its \"nearness\" to other cars. One of the simplest metrics for this is Euclidian distance, defined as the square root of the sum of the difference between each feature squared:\n",
        "\n",
        "![alt text](https://i.stack.imgur.com/2y0bx.png.)\n",
        "\n",
        "Some other approaches include Chi square distance and cosine distance. For further reading on distance functions, see this article: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4978658/.\n",
        "<br></br>\n",
        "\n",
        "Note that **by using Euclidian distance, we are normalizing the data in a way that assumes all features are equally important**. One consequence of this is that values that are generally larger (such as horsepower) will end up having a larger impact on the distance. Unless we have some reason to believe that the larger features are more important, we will want to balance features by **normalizing the data.**\n",
        "\n",
        "\n",
        "One quick and easy way to normalize data is to divide each datum by the maximum value in its category. Let's do that for our data set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aBhWCPsFMn-C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "1eccad32-88f6-4b41-d201-cb94e764dccc"
      },
      "source": [
        "# Normalizing the data by dividing each value by the maximum value in its row. \n",
        "# Remember that we don't normalize the label\n",
        "\n",
        "data_norm_divide = data\n",
        "\n",
        "for i in ['hp','seats','auto']:\n",
        "    data_norm_divide[i] = data[i]/max(data[i].values)\n",
        "    \n",
        "data_norm_divide"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hp</th>\n",
              "      <th>seats</th>\n",
              "      <th>auto</th>\n",
              "      <th>cool</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2008 Honda Civic</th>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2011 Dodge Charger</th>\n",
              "      <td>0.213333</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985 Chevy Blazer</th>\n",
              "      <td>0.255333</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2001 Honda Odyssey</th>\n",
              "      <td>0.140000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017 Bugatti Veyron</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           hp     seats  auto  cool\n",
              "2008 Honda Civic     0.100000  0.714286   0.0   0.0\n",
              "2011 Dodge Charger   0.213333  0.714286   0.0   1.0\n",
              "1985 Chevy Blazer    0.255333  0.428571   1.0   1.0\n",
              "2001 Honda Odyssey   0.140000  1.000000   0.0   0.0\n",
              "2017 Bugatti Veyron  1.000000  0.285714   1.0   NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JXcfmnfzy9u0"
      },
      "source": [
        "As usual, a tool already exists to do this. Using sklearn, we can normalize data with an object called a StandardScaler. We will use this in the example below, but feel free to also read the documentation here: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8WthQEW7RA3",
        "colab_type": "text"
      },
      "source": [
        "Normalization, or more generally \"standardization,\" is a common requirement for machine learning estimators. Beyond shifting values to similar magnitudes, many estimators will run into trouble if the values within individual features are not roughly normally distributed. By **normal distribution,** we mean a Gaussian distribution with 0 mean and unit variance. The sklearn StandardScaler will attempt to scale all the data to approximate a normal distribution for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V6hK0zn1y9u2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "9907f9a1-3dba-411b-f014-1cade5571b6d"
      },
      "source": [
        "# Normalize data by removing the mean and scaling to unit variance from each feature\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "data_unitnorm = pd.DataFrame.from_dict(cars_dict,orient='index')\n",
        "\n",
        "for i in ['hp','seats','auto']:\n",
        "    feature_data = data_unitnorm[i].values.reshape(-1, 1)\n",
        "    scaler.fit(feature_data)\n",
        "    data_unitnorm[i] = scaler.transform(feature_data)\n",
        "    \n",
        "data_unitnorm"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hp</th>\n",
              "      <th>seats</th>\n",
              "      <th>auto</th>\n",
              "      <th>cool</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2008 Honda Civic</th>\n",
              "      <td>-0.724651</td>\n",
              "      <td>0.344124</td>\n",
              "      <td>-0.816497</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2011 Dodge Charger</th>\n",
              "      <td>-0.384908</td>\n",
              "      <td>0.344124</td>\n",
              "      <td>-0.816497</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985 Chevy Blazer</th>\n",
              "      <td>-0.259004</td>\n",
              "      <td>-0.802955</td>\n",
              "      <td>1.224745</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2001 Honda Odyssey</th>\n",
              "      <td>-0.604742</td>\n",
              "      <td>1.491202</td>\n",
              "      <td>-0.816497</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017 Bugatti Veyron</th>\n",
              "      <td>1.973305</td>\n",
              "      <td>-1.376494</td>\n",
              "      <td>1.224745</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           hp     seats      auto  cool\n",
              "2008 Honda Civic    -0.724651  0.344124 -0.816497   0.0\n",
              "2011 Dodge Charger  -0.384908  0.344124 -0.816497   1.0\n",
              "1985 Chevy Blazer   -0.259004 -0.802955  1.224745   1.0\n",
              "2001 Honda Odyssey  -0.604742  1.491202 -0.816497   0.0\n",
              "2017 Bugatti Veyron  1.973305 -1.376494  1.224745   NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C6dLhGxay9u7"
      },
      "source": [
        "Now that the data is standardized, we can begin building our NNA algorithm. First, we will need a function that **calculates the Euclidean distance between two data points**. For our purposes, we will assume the data points will be stored in arrays of the same length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2rayoFQnOyvY",
        "colab": {}
      },
      "source": [
        "# Distance function using formula for euclidean distance\n",
        "\n",
        "def euclidean_dist(datum1, datum2):\n",
        "    inner_val = 0.0\n",
        "    \n",
        "    for g in range(datum1.shape[0]):\n",
        "        inner_val += (datum1[g]- datum2[g]) ** 2\n",
        "    \n",
        "    distance = math.sqrt(inner_val)\n",
        "    return(distance)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6IaktG5_mUNf"
      },
      "source": [
        "Next, we calculate the distance between our Bugatti and each other car using the *euclidean_dist* function. For the sake of testing, we do this twice: once for the normalized data (*data_norm_divide*), and once for the data standardized by sklearn (*data_unitnorm*). Note that we will not input the classification column of cool/uncool to our distance function, which means taking a subset of each array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rY0Wuzz9LZzu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "372ac8ee-81b8-42a1-b6e4-aed03bcec62d"
      },
      "source": [
        "# FYI: This is how you can call a specific row by name and sub-select features\n",
        "bugatti = data.loc[\"2017 Bugatti Veyron\"][[\"hp\",\"seats\",\"auto\"]].values\n",
        "\n",
        "import math\n",
        "\n",
        "# Normalized data by dividing\n",
        "print('Euclidean Distances to 2017 Bugatti Veyron (V1)')\n",
        "for car in range(len(data_norm_divide)):\n",
        "    d = euclidean_dist(data_norm_divide.iloc[4,:3].values, \n",
        "                       data_norm_divide.iloc[car, :3].values)\n",
        "    print('  {}: \\t{:01.3f}'.format(car,d))\n",
        "\n",
        "# Standardized data\n",
        "print('\\nEuclidean Distances to 2017 Bugatti Veyron (V2)')\n",
        "for car in range(len(data_unitnorm)):\n",
        "    d = euclidean_dist(data_unitnorm.iloc[4,:3].values, \n",
        "                       data_unitnorm.iloc[car, :3].values)\n",
        "    print('  {}: \\t{:01.3f}'.format(car,d))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Euclidean Distances to 2017 Bugatti Veyron (V1)\n",
            "  0: \t1.412\n",
            "  1: \t1.343\n",
            "  2: \t0.758\n",
            "  3: \t1.500\n",
            "  4: \t0.000\n",
            "\n",
            "Euclidean Distances to 2017 Bugatti Veyron (V2)\n",
            "  0: \t3.796\n",
            "  1: \t3.562\n",
            "  2: \t2.305\n",
            "  3: \t4.363\n",
            "  4: \t0.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rHRn4t68nIbB"
      },
      "source": [
        "For the normalized data (with each feature divided by its maximum value), we get the following distances:\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Blazer = 0.758\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Odyssey = 1.500\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Civic = 1.412\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Charger = 1.343\n",
        "\n",
        "For the standardized data (using sklearn's StandardScaler), we get the following distances:\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Blazer = 2.305\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Odyssey = 4.363\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Civic = 3.796\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Charger = 3.562"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYfAZW3B7rH-",
        "colab_type": "text"
      },
      "source": [
        "Notice that both techniques yielded the same order of cars from nearest to farthest.\n",
        ">This is coincidental, and unlikely to happen with larger or more varied data sets.\n",
        "\n",
        "Since the distance between the Bugatti and Blazer is the smallest, if $k = 1$, we would classify the Bugatti as cool. However, if $k = 4$ there would no longer be a majority, and we would not be able to classify the Bugatti in either category without a tiebreaker protocol.\n",
        "\n",
        "Generally speaking, *larger values of $k$ reduce noise, but also make the boundaries between classes less distinct*. The best value of $k$ will vary by data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vo5iwH6soijh"
      },
      "source": [
        "# Example kNN "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF6byhXC7xg0",
        "colab_type": "text"
      },
      "source": [
        "Next we will see if we can use k-NNA on the Pima diabetes dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jvkEafL70ug",
        "colab_type": "text"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hQAjk52WQ1da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "40700207-75b6-40e6-a60c-1d5533796ca0"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week1/diabetes.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "data = pd.read_csv(url, names=names)\n",
        "\n",
        "# Dropping NaN rows\n",
        "invalid = ['plas', 'pres', 'skin', 'test', 'mass']\n",
        "\n",
        "for i in invalid:\n",
        "    data[i].replace(to_replace=0, value=np.nan, inplace=True)\n",
        "    \n",
        "data = data.dropna(axis=0).reset_index(drop=True)\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preg</th>\n",
              "      <th>plas</th>\n",
              "      <th>pres</th>\n",
              "      <th>skin</th>\n",
              "      <th>test</th>\n",
              "      <th>mass</th>\n",
              "      <th>pedi</th>\n",
              "      <th>age</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>89.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>78.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.248</td>\n",
              "      <td>26</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>197.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>543.0</td>\n",
              "      <td>30.5</td>\n",
              "      <td>0.158</td>\n",
              "      <td>53</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>189.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>846.0</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.398</td>\n",
              "      <td>59</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   preg   plas  pres  skin   test  mass   pedi  age  class\n",
              "0     1   89.0  66.0  23.0   94.0  28.1  0.167   21      0\n",
              "1     0  137.0  40.0  35.0  168.0  43.1  2.288   33      1\n",
              "2     3   78.0  50.0  32.0   88.0  31.0  0.248   26      1\n",
              "3     2  197.0  70.0  45.0  543.0  30.5  0.158   53      1\n",
              "4     1  189.0  60.0  23.0  846.0  30.1  0.398   59      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lO_knzAAe93n"
      },
      "source": [
        "## Splitting data into Training, Validation, and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b8pRTCC3PatJ",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Columns we will use to make predictions with (features!) feel free to play around with these\n",
        "X_cols = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age']\n",
        "\n",
        "# Column that we want to predict (the labels)\n",
        "y_col = 'class'\n",
        "\n",
        "# 80-20 train-test split of datset\n",
        "test_size = 0.2\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[X_cols], data[y_col], test_size=test_size, random_state=0)\n",
        "\n",
        "# Further split X and y of training into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_size, random_state=0)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qrnVJ8Ovy9vT"
      },
      "source": [
        "## Normalizing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Lt29VKMJy9vT",
        "colab": {}
      },
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "for i in list(X_train):\n",
        "    feature_data_train = X_train[i].values.reshape(-1, 1)\n",
        "    scaler.fit(feature_data_train)\n",
        "    X_train[i] = scaler.transform(feature_data_train)\n",
        "\n",
        "for j in list(X_test):\n",
        "    feature_data_test = X_test[j].values.reshape(-1, 1)\n",
        "    scaler.fit(feature_data_test)\n",
        "    X_test[j] = scaler.transform(feature_data_test)\n",
        "    \n",
        "for k in list(X_val):\n",
        "    feature_data_val = X_val[k].values.reshape(-1, 1)\n",
        "    scaler.fit(feature_data_val)\n",
        "    X_val[k] = scaler.transform(feature_data_val)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7-2ZM8AGIe0S"
      },
      "source": [
        "## Using sklearn's k-NNA\n",
        "\n",
        "Luckily for us, sklearn has some quick and easy functions for normalizing the data, finding Euclidean distances, training, and testing with [k-NNA](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). Try k = 5 to start."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R0FsKkp_qw-x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "30c9d111-3951-4031-d7a4-9a75618b4fb5"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Creating model with sklearn's KNeighborsClassifier -- after running these cells play around with the parameter n!\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Training/fitting a model with training data\n",
        "knn.fit(X_train, y_train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J8y1RuAoy9vr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "4221ba2a-2497-4de4-9019-566ad130d51d"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "y_train_pred=knn.predict(X_train)\n",
        "start = time.time()\n",
        "predictions_fast = knn.predict(X_val)\n",
        "\n",
        "print('Took {} seconds'.format(time.time() - start))\n",
        "print(\"Training Accuracy is \", accuracy_score(y_train, y_train_pred)*100)\n",
        "print(\"Validation Accuracy is \", accuracy_score(y_val,predictions_fast)*100)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Took 0.003973484039306641 seconds\n",
            "Training Accuracy is  79.2\n",
            "Validation Accuracy is  80.95238095238095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w-hHcKgsy9vw"
      },
      "source": [
        "# Conclusions and Limitations\n",
        "\n",
        "k-NNA provides a good baseline classifier for machine learning that is conceptually intuitive and easy to implement and train. One major advantage is that, by standardizing input data, k-NNA can combine any number of features regardless of their original distributions.\n",
        "\n",
        "However, k-NNA can run into problems with more complex data sets:\n",
        "* With additional dimensions, it can be harder to define meaningful distances.\n",
        "* The testing phase can slow down significantly with large data sets since each point's distance is measured to every other point.\n",
        "* \"Majority votes\" may be skewed if one classification is significantly more common than the others.\n",
        "* There is no consideration for correlated features.\n",
        "\n",
        "As a final note, there are other ways to instantiate k-NNA's, and other types of NNA beyond k-Nearest. For example, to solve the large majority problem, we could have used a weighted voting system where nearer neighbors' votes carry more weight."
      ]
    }
  ]
}
